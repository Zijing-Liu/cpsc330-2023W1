{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513d93cb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eca5e1-717f-451f-b23f-ef411de8576f",
   "metadata": {},
   "source": [
    "# CPSC 330 - Applied Machine Learning \n",
    "\n",
    "## Homework 7: Word embeddings and topic modeling \n",
    "**Due date: See the [Calendar](https://htmlpreview.github.io/?https://github.com/UBC-CS/cpsc330/blob/master/docs/calendar.html).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5cb55-0576-4596-ba0e-88d0ad7c39f1",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4651888-484b-42a0-95e1-d273e5069205",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"../../lectures/\"), \"code\"))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from plotting_functions_unsup import *\n",
    "\n",
    "import IPython\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from comat import CooccurrenceMatrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from preprocessing import MyPreprocessor\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b799e3-3d4e-4151-9123-9d3733a63ac3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da966a1-1d24-42a8-b959-e42202e6b824",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "## Submission instructions\n",
    "<hr>\n",
    "rubric={points}\n",
    "\n",
    "**Please be aware that this homework assignment requires installation of several packages in your course environment. It's possible that you'll encounter installation challenges, which might be frustrating. However, remember that solving these issues is not wasting time but it is an essential skill for anyone aspiring to work in data science or machine learning.**\n",
    "\n",
    "Follow the [homework submission instructions](https://github.com/UBC-CS/cpsc330-2023W1/blob/main/docs/homework_instructions.md). \n",
    "\n",
    "**You may work in a group on this homework and submit your assignment as a group.** Below are some instructions on working as a group.  \n",
    "- The maximum group size is 4. \n",
    "- Use group work as an opportunity to collaborate and learn new things from each other. \n",
    "- Be respectful to each other and make sure you understand all the concepts in the assignment well. \n",
    "- It's your responsibility to make sure that the assignment is submitted by one of the group members before the deadline. \n",
    "- You can find the instructions on how to do group submission on Gradescope [here](https://help.gradescope.com/article/m5qz2xsnjy-student-add-group-members).\n",
    "\n",
    "\n",
    "When you are ready to submit your assignment do the following:\n",
    "\n",
    "1. Run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. \n",
    "2. Notebooks with cell execution numbers out of order or not starting from “1” will have marks deducted. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).\n",
    "3. Upload the assignment using Gradescope's drag and drop tool. Check out this [Gradescope Student Guide](https://lthub.ubc.ca/guides/gradescope-student-guide/) if you need help with Gradescope submission.\n",
    "4. Make sure that the plots and output are rendered properly in your submitted file. \n",
    "5. If the .ipynb file is too big and doesn't render on Gradescope, also upload a pdf or html in addition to the .ipynb. If the pdf or html also fail to render on Gradescope, please create two files for your homework: hw6a.ipynb with Exercise 1 and hw6b.ipynb with Exercises 2 and 3 and submit these two files in your submission.  \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be5b2d-1854-4c63-bcc6-9b6258b7293a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b3f00-a3e5-45d8-b504-22a84ace38cd",
   "metadata": {},
   "source": [
    "## Exercise 1:  Exploring pre-trained word embeddings <a name=\"1\"></a>\n",
    "<hr>\n",
    "\n",
    "In lecture 17, we talked about natural language processing (NLP). Using pre-trained word embeddings is very common in NLP. It has been shown that pre-trained word embeddings work well on a variety of text classification tasks. These embeddings are created by training a model like Word2Vec on a huge corpus of text such as a dump of Wikipedia or a dump of the web crawl. \n",
    "\n",
    "A number of pre-trained word embeddings are available out there. Some popular ones are: \n",
    "\n",
    "- [GloVe](https://nlp.stanford.edu/projects/glove/)\n",
    "    * trained using [the GloVe algorithm](https://nlp.stanford.edu/pubs/glove.pdf) \n",
    "    * published by Stanford University \n",
    "- [fastText pre-trained embeddings for 294 languages](https://fasttext.cc/docs/en/pretrained-vectors.html) \n",
    "    * trained using the fastText algorithm\n",
    "    * published by Facebook\n",
    "    \n",
    "In this exercise, you will be exploring GloVe Wikipedia pre-trained embeddings. The code below loads the word vectors trained on Wikipedia using an algorithm called Glove. You'll need `gensim` package in your cpsc330 conda environment to run the code below. \n",
    "\n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda gensim\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4823523-ca44-48a3-94bb-f6e453d27f1c",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader\n",
    "\n",
    "print(list(gensim.downloader.info()[\"models\"].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83e4717e-215b-4c1b-b08a-9f5adbb52467",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    }
   },
   "outputs": [],
   "source": [
    "# This will take a while to run when you run it for the first time.\n",
    "import gensim.downloader as api\n",
    "\n",
    "glove_wiki_vectors = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76ec38c4-ce89-4372-b015-035f4d682132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_wiki_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78dafb-f712-447a-b870-1fac6c249e5f",
   "metadata": {},
   "source": [
    "There are 400,000 word vectors in this pre-trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea1002-2451-4008-aa83-54618c757bb3",
   "metadata": {},
   "source": [
    "Now that we have GloVe Wiki vectors loaded in `glove_wiki_vectors`, let's explore the embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2a75ac-fd18-4a53-89d3-26f1051c4ef3",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8119fb78-d2be-4ccf-8c8d-31026563e072",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Come up with a list of 4 words of your choice and find similar words to these words using `glove_wiki_vectors` embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787fe4d0-4206-4747-9638-7782cca51d3f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6205ebad-49a9-435a-bf84-ae57d29c2068",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "words = [\"coffee\", \"kick\", \"subway\", \"software\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d59251b0-12c0-41cf-867b-3e5cb90f4d8d",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>coffee</th>\n",
       "      <td>(tea, 0.77326899766922)</td>\n",
       "      <td>(drinks, 0.7287518978118896)</td>\n",
       "      <td>(beer, 0.7253385186195374)</td>\n",
       "      <td>(cocoa, 0.7026591300964355)</td>\n",
       "      <td>(wine, 0.7002726793289185)</td>\n",
       "      <td>(drink, 0.6990923881530762)</td>\n",
       "      <td>(corn, 0.6825440526008606)</td>\n",
       "      <td>(sugar, 0.6775094270706177)</td>\n",
       "      <td>(bread, 0.6727856993675232)</td>\n",
       "      <td>(fruit, 0.667149007320404)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kick</th>\n",
       "      <td>(kicks, 0.7849088907241821)</td>\n",
       "      <td>(ball, 0.7723649740219116)</td>\n",
       "      <td>(kicked, 0.76600581407547)</td>\n",
       "      <td>(minute, 0.7423369288444519)</td>\n",
       "      <td>(goal, 0.7406333088874817)</td>\n",
       "      <td>(header, 0.709308922290802)</td>\n",
       "      <td>(deflected, 0.7035992741584778)</td>\n",
       "      <td>(pass, 0.7008392810821533)</td>\n",
       "      <td>(missed, 0.6999455094337463)</td>\n",
       "      <td>(equalizer, 0.6820622682571411)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subway</th>\n",
       "      <td>(subways, 0.7338516712188721)</td>\n",
       "      <td>(train, 0.7133814096450806)</td>\n",
       "      <td>(rail, 0.6966160535812378)</td>\n",
       "      <td>(bus, 0.6924630999565125)</td>\n",
       "      <td>(metro, 0.6873050332069397)</td>\n",
       "      <td>(trains, 0.6864566802978516)</td>\n",
       "      <td>(commuter, 0.6844821572303772)</td>\n",
       "      <td>(tram, 0.6830512881278992)</td>\n",
       "      <td>(transit, 0.652100145816803)</td>\n",
       "      <td>(tunnel, 0.6475099921226501)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>software</th>\n",
       "      <td>(computer, 0.8373122215270996)</td>\n",
       "      <td>(hardware, 0.7876607775688171)</td>\n",
       "      <td>(microsoft, 0.7803657650947571)</td>\n",
       "      <td>(applications, 0.7419033646583557)</td>\n",
       "      <td>(technology, 0.7358859777450562)</td>\n",
       "      <td>(server, 0.7342713475227356)</td>\n",
       "      <td>(user, 0.7320204973220825)</td>\n",
       "      <td>(computers, 0.7276959419250488)</td>\n",
       "      <td>(desktop, 0.7261441349983215)</td>\n",
       "      <td>(web, 0.7209519147872925)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       0                               1  \\\n",
       "coffee           (tea, 0.77326899766922)    (drinks, 0.7287518978118896)   \n",
       "kick         (kicks, 0.7849088907241821)      (ball, 0.7723649740219116)   \n",
       "subway     (subways, 0.7338516712188721)     (train, 0.7133814096450806)   \n",
       "software  (computer, 0.8373122215270996)  (hardware, 0.7876607775688171)   \n",
       "\n",
       "                                        2                                   3  \\\n",
       "coffee         (beer, 0.7253385186195374)         (cocoa, 0.7026591300964355)   \n",
       "kick           (kicked, 0.76600581407547)        (minute, 0.7423369288444519)   \n",
       "subway         (rail, 0.6966160535812378)           (bus, 0.6924630999565125)   \n",
       "software  (microsoft, 0.7803657650947571)  (applications, 0.7419033646583557)   \n",
       "\n",
       "                                         4                             5  \\\n",
       "coffee          (wine, 0.7002726793289185)   (drink, 0.6990923881530762)   \n",
       "kick            (goal, 0.7406333088874817)   (header, 0.709308922290802)   \n",
       "subway         (metro, 0.6873050332069397)  (trains, 0.6864566802978516)   \n",
       "software  (technology, 0.7358859777450562)  (server, 0.7342713475227356)   \n",
       "\n",
       "                                        6                                7  \\\n",
       "coffee         (corn, 0.6825440526008606)      (sugar, 0.6775094270706177)   \n",
       "kick      (deflected, 0.7035992741584778)       (pass, 0.7008392810821533)   \n",
       "subway     (commuter, 0.6844821572303772)       (tram, 0.6830512881278992)   \n",
       "software       (user, 0.7320204973220825)  (computers, 0.7276959419250488)   \n",
       "\n",
       "                                      8                                9  \n",
       "coffee      (bread, 0.6727856993675232)       (fruit, 0.667149007320404)  \n",
       "kick       (missed, 0.6999455094337463)  (equalizer, 0.6820622682571411)  \n",
       "subway     (transit, 0.652100145816803)     (tunnel, 0.6475099921226501)  \n",
       "software  (desktop, 0.7261441349983215)        (web, 0.7209519147872925)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "for word in words:\n",
    "    similar = glove_wiki_vectors.most_similar(word)\n",
    "    result[word] = similar\n",
    "pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b922b29-e591-41a7-b82c-9bcc794d0f10",
   "metadata": {
    "metadata": {
     "tags": [
      "otter_ignore"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1caa0fc7-900a-4ef5-817f-166cafc8cd27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24714da4-3ee5-424e-9a21-bd4b33de2e08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461c1d5-42ff-4267-bf04-e3642c5b40bb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 Word similarity using pre-trained embeddings\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Calculate cosine similarity for the following word pairs (`word_pairs`) using the [`similarity`](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) method of `glove_wiki_vectors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b37d06f-81c7-4120-8dc7-2270105d5ecb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    (\"coast\", \"shore\"),\n",
    "    (\"clothes\", \"closet\"),\n",
    "    (\"old\", \"new\"),\n",
    "    (\"smart\", \"intelligent\"),\n",
    "    (\"dog\", \"cat\"),\n",
    "    (\"tree\", \"lawyer\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77427682-97e3-4fa2-b2d6-84940fce9e27",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7c8c6c4-7113-4cb2-98f8-d34ec1c632e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The similarity between coast and shore is 0.700\n",
      "The similarity between clothes and closet is 0.546\n",
      "The similarity between old and new is 0.643\n",
      "The similarity between smart and intelligent is 0.755\n",
      "The similarity between dog and cat is 0.880\n",
      "The similarity between tree and lawyer is 0.077\n"
     ]
    }
   ],
   "source": [
    "for pair in word_pairs:\n",
    "    print(\n",
    "        \"The similarity between %s and %s is %0.3f\"\n",
    "        % (pair[0], pair[1], glove_wiki_vectors.similarity(pair[0], pair[1]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1a6420-ae43-4469-99e1-59a966238a43",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9186e-ab44-43e5-8a96-7a1c4130b598",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 Representation of all words in English\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. The vocabulary size of Wikipedia embeddings is quite large. The `test_words` list below contains a few new words (called neologisms) and biomedical domain-specific abbreviations. Write code to check whether `glove_wiki_vectors` have representation for these words or not. \n",
    "> If a given word `word` is in the vocabulary, `word in glove_wiki_vectors` will return True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be6a6488-5139-43cb-a88c-b1d45df700cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "test_words = [\n",
    "    \"covididiot\",\n",
    "    \"fomo\",\n",
    "    \"frenemies\",\n",
    "    \"anthropause\",\n",
    "    \"photobomb\",\n",
    "    \"selfie\",\n",
    "    \"pxg\",  # Abbreviation for pseudoexfoliative glaucoma\n",
    "    \"pacg\",  # Abbreviation for primary angle closure glaucoma\n",
    "    \"cct\",  # Abbreviation for central corneal thickness\n",
    "    \"escc\",  # Abbreviation for esophageal squamous cell carcinoma\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfef3be-698f-4792-b7c3-46fac945e7f5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44097c5d-97a0-450e-af88-73c9a4c90c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'covididiot': 'False',\n",
       " 'fomo': 'False',\n",
       " 'frenemies': 'True',\n",
       " 'anthropause': 'False',\n",
       " 'photobomb': 'False',\n",
       " 'selfie': 'False',\n",
       " 'pxg': 'False',\n",
       " 'pacg': 'False',\n",
       " 'cct': 'True',\n",
       " 'escc': 'True'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result = {}\n",
    "word_exists = []\n",
    "words_not_exists = []\n",
    "for word in test_words:\n",
    "    if word in glove_wiki_vectors:\n",
    "      test_result[word] = \"True\" \n",
    "      word_exists.append(word)\n",
    "    else:\n",
    "      test_result[word] = 'False'\n",
    "      words_not_exists.append(word) \n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['frenemies', 'cct', 'escc']\n"
     ]
    }
   ],
   "source": [
    "print(word_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['covididiot', 'fomo', 'anthropause', 'photobomb', 'selfie', 'pxg', 'pacg']\n"
     ]
    }
   ],
   "source": [
    "print(words_not_exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47caea74",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/celine/anaconda3/envs/cpsc330/lib/python3.10/site-packages/nbformat/__init__.py:93: MissingIDFieldWarning:\n",
      "\n",
      "Code cell is missing an id field, this will become a hard error in future nbformat versions. You may want to use `normalize()` on your notebooks before validations (available since nbformat 5.1.4). Previous versions of nbformat are fixing this issue transparently, and will stop doing so in the future.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>q1.3</pre></strong> passed! ✨</p>"
      ],
      "text/plain": [
       "q1.3 results: All test cases passed!"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"q1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2155dbe-979f-4e4d-bd4c-04d2a5f0be20",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea8f10-8bfb-4698-b76c-60f5f8256d5d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Stereotypes and biases in embeddings\n",
    "rubric={points}\n",
    "\n",
    "Word vectors contain lots of useful information. But they also contain stereotypes and biases of the texts they were trained on. In the lecture, we saw an example of gender bias in Google News word embeddings. Here we are using pre-trained embeddings trained on Wikipedia data. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Explore whether there are any worrisome biases or stereotypes present in these embeddings by trying out at least 4 examples. You can use the following two methods or other methods of your choice to explore this. \n",
    "    - the `analogy` function below which gives word analogies (an example shown below)\n",
    "    - [similarity](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=similarity#gensim.models.keyedvectors.KeyedVectors.similarity) or [distance](https://radimrehurek.com/gensim/models/keyedvectors.html?highlight=distance#gensim.models.keyedvectors.KeyedVectors.distances) methods (an example is shown below)\n",
    "\n",
    "> Note that most of the recent embeddings are de-biased. But you might still observe some biases in them. Also, not all stereotypes present in pre-trained embeddings are necessarily bad. But you should be aware of them when you use them in your models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a90c0cd-7cf2-4f82-a617-9a87c526281d",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "def analogy(word1, word2, word3, model=glove_wiki_vectors):\n",
    "    \"\"\"\n",
    "    Returns analogy word using the given model.\n",
    "\n",
    "    Parameters\n",
    "    --------------\n",
    "    word1 : (str)\n",
    "        word1 in the analogy relation\n",
    "    word2 : (str)\n",
    "        word2 in the analogy relation\n",
    "    word3 : (str)\n",
    "        word3 in the analogy relation\n",
    "    model :\n",
    "        word embedding model\n",
    "\n",
    "    Returns\n",
    "    ---------------\n",
    "        pd.dataframe\n",
    "    \"\"\"\n",
    "    print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "    sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "    return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaeec14-64c1-4226-b60f-849489077ddd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examples of using analogy to explore biases and stereotypes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e914a9fa-e1e9-4182-a82d-41737020e44f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : doctor :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.773523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.718943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doctors</td>\n",
       "      <td>0.682433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>patient</td>\n",
       "      <td>0.675068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dentist</td>\n",
       "      <td>0.672603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>0.664246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.652045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nursing</td>\n",
       "      <td>0.645348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mother</td>\n",
       "      <td>0.639333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hospital</td>\n",
       "      <td>0.638750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0        nurse  0.773523\n",
       "1    physician  0.718943\n",
       "2      doctors  0.682433\n",
       "3      patient  0.675068\n",
       "4      dentist  0.672603\n",
       "5     pregnant  0.664246\n",
       "6      medical  0.652045\n",
       "7      nursing  0.645348\n",
       "8       mother  0.639333\n",
       "9     hospital  0.638750"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"doctor\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fceb0128-3c9b-4674-ae40-da3d80f3f00c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14283244"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"aboriginal\", \"success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "774cb08e-95bc-458b-bc78-ee3fcf2b2f7a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3518238"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"white\", \"success\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bef6d-9a57-41a4-8a83-a65d7bc07783",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gender biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a227771e-f8b7-4bad-9881-eb5528f10e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : manager :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>assistant</td>\n",
       "      <td>0.656641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>job</td>\n",
       "      <td>0.626392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>supervisor</td>\n",
       "      <td>0.593479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>owner</td>\n",
       "      <td>0.587832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hired</td>\n",
       "      <td>0.577390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>consultant</td>\n",
       "      <td>0.570659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>director</td>\n",
       "      <td>0.565653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>office</td>\n",
       "      <td>0.564568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>employee</td>\n",
       "      <td>0.561287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ceo</td>\n",
       "      <td>0.556422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0    assistant  0.656641\n",
       "1          job  0.626392\n",
       "2   supervisor  0.593479\n",
       "3        owner  0.587832\n",
       "4        hired  0.577390\n",
       "5   consultant  0.570659\n",
       "6     director  0.565653\n",
       "7       office  0.564568\n",
       "8     employee  0.561287\n",
       "9          ceo  0.556422"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"manager\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6025b7c9-5ffd-4076-8d44-0440d01efe69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13243"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"women\", \"manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "deaa74e9-858e-4b28-a7a8-6b6b522c1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4557554"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"man\", \"manager\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e306e7-26f3-4367-8935-0be49b86be56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man : boss :: woman : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bosses</td>\n",
       "      <td>0.649429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>girlfriend</td>\n",
       "      <td>0.639793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boyfriend</td>\n",
       "      <td>0.623138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>colleague</td>\n",
       "      <td>0.604826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lover</td>\n",
       "      <td>0.593473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>husband</td>\n",
       "      <td>0.590123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ex</td>\n",
       "      <td>0.575455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>friend</td>\n",
       "      <td>0.565926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wife</td>\n",
       "      <td>0.555912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>housekeeper</td>\n",
       "      <td>0.527948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0       bosses  0.649429\n",
       "1   girlfriend  0.639793\n",
       "2    boyfriend  0.623138\n",
       "3    colleague  0.604826\n",
       "4        lover  0.593473\n",
       "5      husband  0.590123\n",
       "6           ex  0.575455\n",
       "7       friend  0.565926\n",
       "8         wife  0.555912\n",
       "9  housekeeper  0.527948"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"man\", \"boss\", \"woman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14743488"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"women\", \"scientist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16145675"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_wiki_vectors.similarity(\"men\", \"scientist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caucasian : professor :: african-american : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lecturer</td>\n",
       "      <td>0.692862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>associate</td>\n",
       "      <td>0.681594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>harvard</td>\n",
       "      <td>0.666835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dean</td>\n",
       "      <td>0.653007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>university</td>\n",
       "      <td>0.651051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>emeritus</td>\n",
       "      <td>0.643217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>yale</td>\n",
       "      <td>0.628395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>faculty</td>\n",
       "      <td>0.626116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>assistant</td>\n",
       "      <td>0.621903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sociology</td>\n",
       "      <td>0.617813</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Analogy word     Score\n",
       "0     lecturer  0.692862\n",
       "1    associate  0.681594\n",
       "2      harvard  0.666835\n",
       "3         dean  0.653007\n",
       "4   university  0.651051\n",
       "5     emeritus  0.643217\n",
       "6         yale  0.628395\n",
       "7      faculty  0.626116\n",
       "8    assistant  0.621903\n",
       "9    sociology  0.617813"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"caucasian\", \"professor\", \"african-american\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caucasian : doctor :: african-american : ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Analogy word</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>physician</td>\n",
       "      <td>0.653751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dr.</td>\n",
       "      <td>0.620225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>medical</td>\n",
       "      <td>0.615414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nurse</td>\n",
       "      <td>0.601160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>psychologist</td>\n",
       "      <td>0.575256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>assistant</td>\n",
       "      <td>0.572545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>teacher</td>\n",
       "      <td>0.567794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>psychiatrist</td>\n",
       "      <td>0.559851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>author</td>\n",
       "      <td>0.559437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>surgeon</td>\n",
       "      <td>0.553548</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Analogy word     Score\n",
       "0     physician  0.653751\n",
       "1           dr.  0.620225\n",
       "2       medical  0.615414\n",
       "3         nurse  0.601160\n",
       "4  psychologist  0.575256\n",
       "5     assistant  0.572545\n",
       "6       teacher  0.567794\n",
       "7  psychiatrist  0.559851\n",
       "8        author  0.559437\n",
       "9       surgeon  0.553548"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(\"caucasian\", \"doctor\", \"african-american\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474b6ac-25f3-45f1-97db-bfadf71d9f80",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Discussion\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Discuss your observations from 1.4. Are there any worrisome biases in these embeddings trained on Wikipedia?   \n",
    "2. Give an example of how using embeddings with biases could cause harm in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e95ea8-80c0-4184-8b89-1fa1581404f1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc56e7e",
   "metadata": {},
   "source": [
    "**1. Observations**\n",
    "1. In the example provided, the \"white\" word is much closer to the word \"success\" than \"aboriginal\". \n",
    "2. There are also career-wide gender biases in the embeddings. In the analogy analysis, man to manager is mostly like women to \"assistant\".  What's worse, the analogy result also shows that man is to boss as woman is to girlfriend. Also, \"man\" has a much higher similarity score to \"manager\" than \"woman\" has, which is a fair representation of the real workforce where managers are dominated by males in most industries.\n",
    "3. Another bias in the embedding is on race. When Caucasian is to professor, African-American is to lecturer. When Caucasian is to doctor, the most similar analogy is African-American to physicians. These results indicates that Caucasian is more associated to higher-paid and higher-level positions in the medical and academic professions than African-American.\n",
    "\n",
    "**2.Impacts**\n",
    "\n",
    "1. \"white\" is much closer to word \"success\" than \"aboriginal\", this biased representation can put aboriginal people in a disadvantaged position in their career development. If a company chooses to use an AI resume screening tool based on biased word embeddings, aboriginal or visual minority candidates may have a lower chance of getting selected for an interview than white candidates. \n",
    "2. Gender bias on professionals can enforce people's conception model, discouraging females from advancing their careers to the next level. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a6ab6-62ef-4fdd-ba0d-5e7e920154a3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cb13f7-e08c-4f8d-86c9-b1602cc8a5b4",
   "metadata": {},
   "source": [
    "## Exercise 2: Topic modeling \n",
    "\n",
    "The goal of topic modeling is discovering high-level themes in a large collection of texts. \n",
    "\n",
    "In this homework, you will explore topics in [the 20 newsgroups text dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html) using `scikit-learn`'s `LatentDirichletAllocation` (LDA) model. \n",
    "\n",
    "Usually, topic modeling is used for discovering abstract \"topics\" that occur in a collection of documents when you do not know the actual topics present in the documents. But 20 newsgroups text dataset is labeled with categories (e.g., sports, hardware, religion), and you will be able to cross-check the topics discovered by your model with these available topics. \n",
    "\n",
    "The starter code below loads the train and test portion of the data and convert the train portion into a pandas DataFrame. For speed, we will only consider documents with the following 8 categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "327ca91e-cf57-4481-b4cc-46c29a9849a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9afa3b30-7f32-48ec-8291-69c964a38c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You know, I was reading 18 U.S.C. 922 and some...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n\\n\\nIt's not a bad question: I don't have an...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4558</th>\n",
       "      <td>Hi Everyone ::\\n\\nI am  looking for  some soft...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4559</th>\n",
       "      <td>Archive-name: x-faq/part3\\nLast-modified: 1993...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4560</th>\n",
       "      <td>\\nThat's nice, but it doesn't answer the quest...</td>\n",
       "      <td>6</td>\n",
       "      <td>talk.politics.guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4561</th>\n",
       "      <td>Hi,\\n     I just got myself a Gateway 4DX-33V ...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>\\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4563 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "0     You know, I was reading 18 U.S.C. 922 and some...       6   \n",
       "1     \\n\\n\\nIt's not a bad question: I don't have an...       1   \n",
       "2     \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3     The following problem is really bugging me,\\na...       2   \n",
       "4     \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "...                                                 ...     ...   \n",
       "4558  Hi Everyone ::\\n\\nI am  looking for  some soft...       1   \n",
       "4559  Archive-name: x-faq/part3\\nLast-modified: 1993...       2   \n",
       "4560  \\nThat's nice, but it doesn't answer the quest...       6   \n",
       "4561  Hi,\\n     I just got myself a Gateway 4DX-33V ...       2   \n",
       "4562  \\n\\n[h] \\tThe Armenians in Nagarno-Karabagh ar...       7   \n",
       "\n",
       "                target_name  \n",
       "0        talk.politics.guns  \n",
       "1             comp.graphics  \n",
       "2             comp.graphics  \n",
       "3            comp.windows.x  \n",
       "4     talk.politics.mideast  \n",
       "...                     ...  \n",
       "4558          comp.graphics  \n",
       "4559         comp.windows.x  \n",
       "4560     talk.politics.guns  \n",
       "4561         comp.windows.x  \n",
       "4562  talk.politics.mideast  \n",
       "\n",
       "[4563 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats = [\n",
    "    \"rec.sport.hockey\",\n",
    "    \"rec.sport.baseball\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"alt.atheism\",\n",
    "    \"comp.graphics\",\n",
    "    \"comp.windows.x\",\n",
    "    \"talk.politics.mideast\",\n",
    "    \"talk.politics.guns\",\n",
    "]  # We'll only consider these categories out of 20 categories for speed.\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(\n",
    "    subset=\"train\", remove=(\"headers\", \"footers\", \"quotes\"), categories=cats\n",
    ")\n",
    "X_news_train, y_news_train = newsgroups_train.data, newsgroups_train.target\n",
    "df = pd.DataFrame(X_news_train, columns=[\"text\"])\n",
    "df[\"target\"] = y_news_train\n",
    "df[\"target_name\"] = [\n",
    "    newsgroups_train.target_names[target] for target in newsgroups_train.target\n",
    "]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8095c853-d4d5-49a3-bda0-129ffd2f690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.windows.x',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cdcfc-7545-4a40-a9b3-34b58bb38365",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e21b066-8de6-42a7-8e4e-097fd8727367",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Preprocessing using [spaCy](https://spacy.io/)\n",
    "rubric={points}\n",
    "\n",
    "Preprocessing is a crucial step before carrying out topic modeling and it markedly affects topic modeling results. In this exercise, you'll prepare the data using [spaCy](https://spacy.io/) for topic modeling. \n",
    "\n",
    "**Your tasks:** \n",
    "\n",
    "- Write code using [spaCy](https://spacy.io/) to preprocess the `text` column in the given dataframe `df` and save the processed text in a new column called `text_pp` within the same dataframe.\n",
    "\n",
    "If you do not have spaCy in your course environment, you'll have to [install it](https://spacy.io/usage) and download the pretrained model en_core_web_md. \n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "\n",
    "Note that there is no such thing as \"perfect\" preprocessing. You'll have to make your own judgments and decisions on which tokens are likely to be more informative for the given task. Some common text preprocessing steps for topic modeling include: \n",
    "- getting rid of slashes, new-line characters, or any other non-informative characters\n",
    "- sentence segmentation and tokenization      \n",
    "- replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "- getting rid of other fairly unique tokens which are not going to help us in topic modeling  \n",
    "- excluding stopwords and punctuation \n",
    "- lemmatization\n",
    "\n",
    "\n",
    "> Check out [these available attributes](https://spacy.io/api/token#attributes) for `token` in spaCy which might help you with preprocessing. \n",
    "\n",
    "> You can also get rid of words with specific POS tags. [Here](https://universaldependencies.org/u/pos/) is the list of part-of-speech tags used in spaCy. \n",
    "\n",
    "> You may have to use regex to clean text before passing it to spaCy. Also, you might have to go back and forth between preprocessing in this exercise and and topic modeling in Exercise 2 before finalizing preprocessing steps. \n",
    "\n",
    "> Note that preprocessing the corpus might take some time. So here are a couple of suggestions: 1) During the debugging phase, work on a smaller subset of the data. 2) Once you finalize the preprocessing part, you might want to save the preprocessed data in a CSV and work with this CSV so that you don't run the preprocessing part every time you run the notebook. \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6816e7a3-d6d4-4bef-81f4-9b4fdf638175",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0036dbe6-2353-4c5a-80bf-2b884a170a29",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/celine/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "stop_words = list(set(stopwords.words(\"english\")))\n",
    "punctuation = string.punctuation\n",
    "stop_words += list(punctuation)\n",
    "stop = [\"\\n\", \"\\t\"]\n",
    "stop_words += stop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8be7c802-d10b-4929-9664-34274299e884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(\n",
    "    doc,\n",
    "    min_token_len=2,\n",
    "    irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\", \"INTJ\", \"VERB\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Given text, min_token_len, and irrelevant_pos carry out preprocessing of the text\n",
    "    and return a preprocessed string.\n",
    "\n",
    "    Parameters\n",
    "    -------------\n",
    "    doc : (spaCy doc object)\n",
    "        the spacy doc object of the text\n",
    "    min_token_len : (int)\n",
    "        min_token_length required\n",
    "    irrelevant_pos : (list)\n",
    "        a list of irrelevant pos tags\n",
    "\n",
    "    Returns\n",
    "    -------------\n",
    "    (str) the preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    clean_text = []\n",
    "\n",
    "    for token in doc:\n",
    "        if (token.like_url):\n",
    "            clean_text.append(\"URL\".lower())\n",
    "            continue\n",
    "        if (token.like_num):\n",
    "            clean_text.append(\"NUM\".lower())\n",
    "            continue\n",
    "        if (token.like_email):\n",
    "            clean_text.append(\"EMAIL\".lower())\n",
    "            continue\n",
    "        if (\n",
    "            token not in stop_words  # Check if it's not a stop_words\n",
    "            and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "            and token.pos_ not in irrelevant_pos\n",
    "        ):  # Check if the POS is in the acceptable POS tags\n",
    "            lemma = token.lemma_  # Take the lemma of the word\n",
    "            clean_text.append(lemma.lower())\n",
    "    return \" \".join(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_pp\"] = [preprocess(text) for text in nlp.pipe(df[\"text\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c2b633b-6601-4b83-8668-800a99d3ba20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>target_name</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nActuallay I don't, but on the other hand I d...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>actuallay other hand idea num newsgroup aspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The following problem is really bugging me,\\na...</td>\n",
       "      <td>2</td>\n",
       "      <td>comp.windows.x</td>\n",
       "      <td>problem would help num window child event_mask...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n\\n  This is the latest from UPI \\n\\n     For...</td>\n",
       "      <td>7</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>late upi foreign ministry spokesman ferhat ata...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hi,\\n   I'd like to subscribe to Leadership Ma...</td>\n",
       "      <td>5</td>\n",
       "      <td>soc.religion.christian</td>\n",
       "      <td>leadership magazine num disk paper disk would ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "2  \\nActuallay I don't, but on the other hand I d...       1   \n",
       "3  The following problem is really bugging me,\\na...       2   \n",
       "4  \\n\\n  This is the latest from UPI \\n\\n     For...       7   \n",
       "5  Hi,\\n   I'd like to subscribe to Leadership Ma...       5   \n",
       "\n",
       "              target_name                                            text_pp  \n",
       "2           comp.graphics  actuallay other hand idea num newsgroup aspect...  \n",
       "3          comp.windows.x  problem would help num window child event_mask...  \n",
       "4   talk.politics.mideast  late upi foreign ministry spokesman ferhat ata...  \n",
       "5  soc.religion.christian  leadership magazine num disk paper disk would ...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[2:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bbad26-84c1-41ed-8201-0843924b3166",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980f9bd5-31ef-43dd-9ec7-e0e1cd9a9714",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Justification\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Outline the preprocessing steps you carried out in the previous exercise (bullet point format is fine), providing a brief justification when necessary. \n",
    "\n",
    "> You might want to wait to answer this question till you are done with Exercise 2 and you have finalized the preprocessing steps in 2.1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8df885-0fb7-4ec7-bef6-bb0c3cd82e9e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56baec5e",
   "metadata": {},
   "source": [
    "- sentence segmentation and tokenization     \n",
    "- getting rid of slashes, new-line characters including \"\\n\" and \"\\t\", and other unique characters like '-', '|'\n",
    "- replacing urls, email addresses, or numbers with generic tokens such as \"URL\",  \"EMAIL\", \"NUM\". \n",
    "- excluding english stopwords and punctuation \n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c22f0e-2526-4f83-94ee-a625c7a5ed04",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f080f91c-6801-4b89-9fbc-1b574b09915c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Build a topic model using sklearn's LatentDirichletAllocation\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Build LDA models on the preprocessed data using using [sklearn's `LatentDirichletAllocation`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html) and random state 42. Experiment with a few values for the number of topics (`n_components`). Pick a reasonable number for the number of topics and briefly justify your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e472711-3c92-4b82-a31d-5d4bf55dbf2a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05573f4c",
   "metadata": {},
   "source": [
    "transform raw text data using BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4563x33574 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 202468 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#stop_words += [\"num\", \"url\", \"email\"]\n",
    "vec = CountVectorizer(stop_words=\"english\")\n",
    "X = vec.fit_transform(df[\"text_pp\"])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target_name'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I will try the setting the n_components to be the number of target_names 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "n_topics = 8 # number of topics\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=42\n",
    ")\n",
    "lda.fit(X) \n",
    "document_topics = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "num           num           num           num           num           \n",
      "god           file          people        argument      game          \n",
      "people        url           gun           fallacy       year          \n",
      "jesus         program       armenian      san           team          \n",
      "thing         email         armenians     son           good          \n",
      "bible         window        turkish       marriage      player        \n",
      "church        entry         time          converter     time          \n",
      "good          server        government    church        season        \n",
      "time          widget        year          adl           point         \n",
      "life          line          law           example       hockey        \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       \n",
      "--------      --------      --------      \n",
      "israel        num           image         \n",
      "israeli       email         num           \n",
      "right         url           jpeg          \n",
      "jews          new           problem       \n",
      "arab          period        thank         \n",
      "people        file          time          \n",
      "num           image         program       \n",
      "state         available     color         \n",
      "peace         play          question      \n",
      "law           goal          gif           \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(8),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the LDA result with n_components = 8. Four broader topics can be summarized\n",
    "- Topic2 and topic5 are both about politics\n",
    "- Topic0 and topic 3 are both about religions\n",
    "- Topic1 and topic6 and topic 7are both related to computers\n",
    "- Topic4 and is around sports\n",
    "\n",
    "Therefore, I'll try to reduce n_components to 4 and perform LDA modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88908be8-85df-40bf-9d6d-0b48948a7137",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_topics = 4 # number of topics\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=42\n",
    ")\n",
    "lda.fit(X) \n",
    "document_topics = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bed16e6f-2706-463e-80cc-c64d2b75a296",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04532734, 0.00670405, 0.94103388, 0.00693474],\n",
       "       [0.53609737, 0.45568878, 0.00403964, 0.00417421],\n",
       "       [0.07845799, 0.25296599, 0.00617735, 0.66239866],\n",
       "       ...,\n",
       "       [0.00692611, 0.00687999, 0.97857677, 0.00761713],\n",
       "       [0.00829253, 0.97512931, 0.00835022, 0.00822794],\n",
       "       [0.00103815, 0.00104406, 0.99686996, 0.00104783]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "num           num           num           num           \n",
      "game          url           people        god           \n",
      "team          file          gun           people        \n",
      "year          email         time          good          \n",
      "player        program       armenian      thing         \n",
      "good          image         right         question      \n",
      "season        window        law           church        \n",
      "hockey        available     year          way           \n",
      "league        server        israel        true          \n",
      "period        entry         jews          argument      \n",
      "point         widget        armenians     time          \n",
      "new           version       day           religion      \n",
      "time          line          government    bible         \n",
      "goal          application   turkish       atheist       \n",
      "play          information   man           christian     \n",
      "nhl           color         thing         belief        \n",
      "fan           thank         jesus         faith         \n",
      "email         code          state         life          \n",
      "run           motif         way           reason        \n",
      "shot          format        weapon        word          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vec.get_feature_names_out())\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(4),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04532734, 0.00670405, 0.94103388, 0.00693474])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text_pp']\n",
    "document_topics[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f4ff43-188c-4d9a-8404-691cd563dd9a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 Exploring word topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For the number of topics you picked in the previous exercise, show top 10 words for each of your topics and suggest labels for each of the topics (similar to how we came up with labels \"health and nutrition\", \"fashion\", and \"machine learning\" in the toy example we saw in class). \n",
    "\n",
    "> If your topics do not make much sense, you might have to go back to preprocessing in Exercise 2.1, improve it, and train your LDA model again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbe9de-c9d9-41a1-bf5a-10220ce6fa38",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       \n",
      "--------      --------      --------      --------      \n",
      "num           num           num           num           \n",
      "game          url           people        god           \n",
      "team          file          gun           people        \n",
      "year          email         time          good          \n",
      "player        program       armenian      thing         \n",
      "good          image         right         question      \n",
      "season        window        law           church        \n",
      "hockey        available     year          way           \n",
      "league        server        israel        true          \n",
      "period        entry         jews          argument      \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "mglearn.tools.print_topics(\n",
    "    topics=range(4),\n",
    "    feature_names=feature_names,\n",
    "    sorting=sorting,\n",
    "    topics_per_chunk=5,\n",
    "    n_words=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here is how we can interpret the topics\n",
    "    - Topic 0 $\\rightarrow$ sports\n",
    "    - Topic 1 $\\rightarrow$ computers programs, graphs, windows\n",
    "    - Topic 2 $\\rightarrow$ politics\n",
    "    - Topic 3 $\\rightarrow$ religions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4cbbb9-1057-43ef-ac63-842e3e4cb43c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d53117b-897c-4eba-b1bd-dfedcb3d35ff",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Exploring document topic association\n",
    "rubric={points}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Show the document topic assignment of the first five documents from `df`. \n",
    "2. Comment on the document topic assignment of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12934a67-c48d-4c42-a856-248615c6202b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1f09783a-5644-46ac-bb2f-d60947a13d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>You know, I was reading 18 U.S.C. 922 and something just did not make \\nsence and I was wondering if someone could help me out.\\n\\nSay U.S.C. 922 :\\n\\n(1) Except as provided in paragraph (2), it shall be unlawful for\\nany person to transfer or possess a machinegun.\\n\\n Well I got to looking in my law dictionary and I found that a \"person\" \\nmight also be an artificial entity that is created by government \\nand has no rights under the federal constitution. So, what I \\ndon't understand is how a statute like 922 can be enforced on \\nan individual. So someone tell me how my government can tell\\nme what I can or cannot possess. Just passing a law \\ndoes not make it LAW. Everyone knows that laws are constitional\\nuntil it goes to court. So, has it ever gone to court, not\\njust your run of the mill \"Ok I had it I am guilty, put me in jail\"\\n\\nHas anyone ever claimed that they had a right to possess and was told\\nby the Supreme Court that they didn't have that right?\\n\\n\\n</th>\n",
       "      <td>0.045327</td>\n",
       "      <td>0.006704</td>\n",
       "      <td>0.941034</td>\n",
       "      <td>0.006935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\n\\nIt's not a bad question: I don't have any refs that list this algorithm\\neither. But thinking about it a bit, it shouldn't be too hard.\\n\\n1) Take three of the points and find the plane they define as well as\\nthe circle that they lie on (you say you have this algorithm already)\\n\\n2) Find the center  of this circle. The line passing through this center\\nperpendicular to the plane of the three points passes through the center of\\nthe sphere.\\n\\n3) Repeat with the unused point and two of the original points. This\\ngives you two different lines that both pass through the sphere's\\norigin. Their interection is the center of the sphere.\\n\\n4) the radius is easy to compute, it's just the distance from the center to\\nany of the original points.\\n\\nI'll leave the math to you, but this is a workable algorithm. :-)\\n\\n\\nAn alternate method would be to take pairs of points: the plane formed\\nby the perpendicular bisector of each line segment pair also contains the\\ncenter of the sphere. Three pairs will form three planes, intersecting\\nat a point. This might be easier to implement.</th>\n",
       "      <td>0.536097</td>\n",
       "      <td>0.455689</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.004174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\nActuallay I don't, but on the other hand I don't support the idea of having\\none newsgroup for every aspect of graphics programming as proposed by Brian,\\nin his reply to my original posting.\\nI would suggest a looser structure more like a comp.graphics.programmer,\\ncomp.graphics.hw_and_sw\\nThe reason for making as few groups as possible is for the same reason you\\nsay we shouldn't spilt up, not to get to few postings every day.\\nI takes to much time to browse through all postings just to find two or \\nthree I'm interested in.\\n\\nI understand and agree when you say you want all aspects of graphics in one\\nmeeting. I agree to some extension. I see news as a forum to exchange ideas,\\nhelp others or to be helped. I think this is difficult to achive if there\\nare so many different things in one meeting.\\n\\nGood evening netters|-)</th>\n",
       "      <td>0.078458</td>\n",
       "      <td>0.252966</td>\n",
       "      <td>0.006177</td>\n",
       "      <td>0.662399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The following problem is really bugging me,\\nand I would appreciate any help.\\n\\nI create two windows:\\n\\nw1 (child to root) with event_mask = ButtonPressMask|KeyPressMask;\\nw2 (child to w1) with do_not_propagate_mask = ButtonPressMask|KeyPressMask;\\n\\n\\nKeypress events in w2 are discarded, but ButtonPress events fall through\\nto w1, with subwindow set to w2.\\n\\nFYI, I'm using xnews/olvwm.\\n\\nAm I doing something fundamentally wrong here?</th>\n",
       "      <td>0.011576</td>\n",
       "      <td>0.921273</td>\n",
       "      <td>0.055135</td>\n",
       "      <td>0.012015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\\n\\n  This is the latest from UPI \\n\\n     Foreign Ministry spokesman Ferhat Ataman told journalists Turkey was\\n     closing its air space to all flights to and from Armenia and would\\n     prevent humanitarian aid from reaching the republic overland across\\n     Turkish territory.\\n\\n  \\n   Historically even the most uncivilized of peoples have exhibited \\n   signs of compassion by allowing humanitarian aid to reach civilian\\n   populations. Even the Nazis did this much.\\n\\n   It seems as though from now on Turkey will publicly pronounce \\n   themselves 'hypocrites' should they choose to continue their\\n   condemnation of the Serbians.\\n\\n\\n\\n--</th>\n",
       "      <td>0.007617</td>\n",
       "      <td>0.007715</td>\n",
       "      <td>0.976996</td>\n",
       "      <td>0.007672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           0         1  \\\n",
       "You know, I was reading 18 U.S.C. 922 and somet...  0.045327  0.006704   \n",
       "\\n\\n\\nIt's not a bad question: I don't have any...  0.536097  0.455689   \n",
       "\\nActuallay I don't, but on the other hand I do...  0.078458  0.252966   \n",
       "The following problem is really bugging me,\\nan...  0.011576  0.921273   \n",
       "\\n\\n  This is the latest from UPI \\n\\n     Fore...  0.007617  0.007715   \n",
       "\n",
       "                                                           2         3  \n",
       "You know, I was reading 18 U.S.C. 922 and somet...  0.941034  0.006935  \n",
       "\\n\\n\\nIt's not a bad question: I don't have any...  0.004040  0.004174  \n",
       "\\nActuallay I don't, but on the other hand I do...  0.006177  0.662399  \n",
       "The following problem is really bugging me,\\nan...  0.055135  0.012015  \n",
       "\\n\\n  This is the latest from UPI \\n\\n     Fore...  0.976996  0.007672  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = {}\n",
    "for i in range(5):\n",
    "    result[df[\"text\"].iloc[i]] =  document_topics[i]\n",
    "pd.DataFrame(result).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e21cac-5f1a-4480-b683-47c3b41a1199",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc0 is associated to topic2(politics) and the model is very confident about this classification with an associate score of 94%. This is well aligned with the semantic meanings of the real text\n",
    "- Doc1 is 54% associated with topic0 (sports) and 46% associate with topic1(computers). In fact, Doc1 is under the topic of computer graphics and the LDA model didn't perform well on this document.\n",
    "- Doc2 is 66% related to topic3(religion), however, the actual theme of Doc2 should be computer graphics\n",
    "- Doc 3 is 92% related to topic1, which is true\n",
    "- Doc 4 is 97% related to topic2 politics, which is also an accurate classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42685c5-01cb-4495-aae6-f803d6f75172",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "## Exercise 3: Short answer questions \n",
    "<hr>\n",
    "\n",
    "rubric={points}\n",
    "\n",
    "1. Briefly explain how content-based filtering works in the context of recommender systems. \n",
    "2. Discuss at least two negative consequences of recommender systems.\n",
    "3. What is transfer learning in natural language processing or computer vision? Briefly explain.     \n",
    "4. In lecture 18, we talked about multi-class classification. Comment on how each model in the list below might be handling multiclass classification. Check `scikit-learn` documentation for each of these models when you answer this question.  \n",
    "    - Decision Tree\n",
    "    - KNN\n",
    "    - Random Forest    \n",
    "    - Logistic Regression\n",
    "    - SVM RBF\n",
    "5. In Lecture 18, we briefly discussed how neural networks are sort of like `Pipeline`s, in the sense that they involve multiple sequential transformations of the data, finally resulting in the prediction. Why was this property useful when it came to transfer learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01807641-531e-454a-9374-fc8ad04bc30b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce200f",
   "metadata": {},
   "source": [
    "1. Content-based filtering is supervised machine learning, it works by analyzing the attributes or properties of items that a user has previously interacted with or liked, thus to recommend new items based on user behaviors the model has learned.\n",
    "\n",
    "2. Recommendation system can impose biases to a lager user group, which can undermine equity movements within society. Additionally, they tend to amplify the popularity of already well-known content, thereby reducing the visibility and discoverability of lesser-known yet valuable content.\n",
    "\n",
    "3. In NLP, transfer learning involves using a pre-trained language model that has been trained on a large corpus of text data, to perform new tasks (such as sentiment analysis, text classification) within the text data.\n",
    "\n",
    "    Similarly, in computer vision, transfer learning entails using pre-trained convolutional neural network (CNN) models that have been trained on large-scale image datasets and fine-tuning them on new, smaller datasets for tasks like object detection, image classification, or image segmentation\n",
    "\n",
    "4.  - Decision Tree: scikit-learn's DecisionTreeClassifier inherently supports multi-class classification. In case that there are multiple classes with the same and highest probability, the classifier will predict the class with the lowest index amongst those classes.\n",
    "\n",
    "    - KNN: KNN can perform multi-class classification because the majority class among the k-nearest neighbor is taken to be the class for the encountered example.\n",
    "    - Random Forest: since random is made of a series of decision trees, it inherently supports multi-class.\n",
    "    - Logistic Regression: Can apply to multi-class classification with the one-vs.-rest approach, train multiple binary classifiers. When a new example is encountered, we calculate the raw score of all binary classifiers, can pick the highest raw score.\n",
    "    - SVM RBF: The multiclass support is handled according to a one-vs-one scheme.\n",
    "\n",
    "\n",
    "\n",
    "5. neural network y extract hierarchical representations from raw data in many layers, the learned features from earlier layers can be useful for other tasks. It can also adapt to different tasks by adjusting their weights during fine-tuning allows for better generalization to new tasks or domains. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12929238-7f2c-421e-bb9a-32f0debf0636",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d6d11-0ef5-4315-9854-f64a427e62af",
   "metadata": {},
   "source": [
    "**Before submitting your assignment, please make sure you have followed all the instructions in the Submission instructions section at the top.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5491f1d3-dd84-4ed2-b2b3-9148ac8df7a5",
   "metadata": {},
   "source": [
    "![](img/eva-well-done.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpsc330",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.3": {
     "name": "q1.3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> a = 3\n>>> assert a == 3, 'All good'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
