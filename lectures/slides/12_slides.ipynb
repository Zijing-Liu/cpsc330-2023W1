{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953b12fc",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "import seaborn as sns\n",
    "from plotting_functions import *\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea3df0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 12: Feature importances and model transparency\n",
    "\n",
    "UBC 2023-24\n",
    "\n",
    "Instructor: Varada Kolhatkar and Andrew Roth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268e18",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Announcements \n",
    "\n",
    "- Midterm is next week.  \n",
    "    - Bring your laptop. Make sure that it's fully charged. \n",
    "    - Bring your UBC ID Card. \n",
    "- HW5 is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6418583",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Interpret the coefficients of linear regression for ordinal, one-hot encoded categorical, and scaled numeric features. \n",
    "- Explain why interpretability is important in ML.\n",
    "- Use `feature_importances_` attribute of `sklearn` models and interpret its output. \n",
    "- Apply SHAP to assess feature importances and interpret model predictions. \n",
    "- Explain force plot, summary plot, and dependence plot produced with shapely values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2763536",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0334059",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I'm using `seaborn` in this lecture for easy heatmap plotting, which is not in the course environment. You can install it as follows. \n",
    "```\n",
    "> conda activate cpsc330\n",
    "> conda install -c anaconda seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f208bca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data \n",
    "\n",
    "In the first part of this lecture, we'll be using [Kaggle House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/), the dataset we used in lecture 10. As usual, to run this notebook you'll need to download the data. Unzip the data into a subdirectory called `data`. For this dataset, train and test have already been separated. We'll be working with the train portion in this lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be9eb04",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/housing-kaggle/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/housing-kaggle/train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m train_test_split(df, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m)\n\u001b[1;32m      3\u001b[0m train_df\u001b[38;5;241m.\u001b[39mhead(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/cpsc330/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cpsc330/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/cpsc330/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/cpsc330/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/cpsc330/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/housing-kaggle/train.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/housing-kaggle/train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, random_state=123)\n",
    "train_df.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476730ac",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The prediction task is predicting `SalePrice` given features related to properties.  \n",
    "- Note that the target is numeric, not categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3999278",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"SalePrice\"])\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"SalePrice\"])\n",
    "y_test = test_df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a62e23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "drop_features = [\"Id\"]\n",
    "numeric_features = [\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "    \"YrSold\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da21992",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_reg = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "ordering = [\n",
    "    \"Po\",\n",
    "    \"Fa\",\n",
    "    \"TA\",\n",
    "    \"Gd\",\n",
    "    \"Ex\",\n",
    "]  # if N/A it will just impute something, per below\n",
    "ordering_ordinal_reg = [ordering] * len(ordinal_features_reg)\n",
    "ordering_ordinal_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420c7d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_oth = [\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"Functional\",\n",
    "    \"Fence\",\n",
    "]\n",
    "ordering_ordinal_oth = [\n",
    "    [\"NA\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    [\"NA\", \"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4db98",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = list(\n",
    "    set(X_train.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(ordinal_features_reg)\n",
    "    - set(ordinal_features_oth)\n",
    "    - set(drop_features)\n",
    ")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee6c89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "ordinal_transformer_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_reg),\n",
    ")\n",
    "\n",
    "ordinal_transformer_oth = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_oth),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68607db1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer_reg, ordinal_features_reg),\n",
    "    (ordinal_transformer_oth, ordinal_features_oth),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c2c9dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lr_pipe = make_pipeline(preprocessor, Ridge())\n",
    "scores = cross_validate(lr_pipe, X_train, y_train, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de0aa9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ed7be0",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- How does the output depend upon the input? \n",
    "- How do the predictions change as a function of a particular feature?\n",
    "- If the model has poor interpretability does not make sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c910df7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SimpleFeature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de64639",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's look at the correlations between various features with other features and the target in our encoded data (first row/column). \n",
    "- In simple terms here is how you can interpret correlations between two variables $X$ and $Y$:\n",
    "  - If $Y$ goes up when $X$ goes up, we say $X$ and $Y$ are positively correlated.\n",
    "  - If $Y$ goes down when $X$ goes up, we say $X$ and $Y$ are negatively correlated.\n",
    "  - If $Y$ is unchanged when $X$ changes, we say $X$ and $Y$ are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8789ca6",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "preprocessor.fit(X_train)\n",
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38818f83",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "new_columns = (\n",
    "    numeric_features + ordinal_features_reg + ordinal_features_oth + ohe_columns\n",
    ")\n",
    "X_train_enc = pd.DataFrame(\n",
    "    preprocessor.transform(X_train), index=X_train.index, columns=new_columns\n",
    ")\n",
    "X_train_enc.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e206062d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cor = pd.concat((y_train, X_train_enc), axis=1).iloc[:, :10].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.set(font_scale=0.8)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4483bf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can immediately see that `SalePrice` is highly correlated with `OverallQual`.\n",
    "- This is an early hint that `OverallQual` is a useful feature in predicting `SalePrice`.\n",
    "- However, this approach is **extremely simplistic**.\n",
    "  - It only looks at each feature in isolation.\n",
    "  - It only looks at linear associations:\n",
    "    - What if `SalePrice` is high when `BsmtFullBath` is 2 or 3, but low when it's 0, 1, or 4? They might seem uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb846c6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cor = pd.concat((y_train, X_train_enc), axis=1).iloc[:, 10:15].corr()\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.set(font_scale=0.8)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd16d2a",
   "metadata": {},
   "source": [
    "- Looking at this diagram also tells us the relationship between features. \n",
    "  - For example, `1stFlrSF` and `TotalBsmtSF` are highly correlated. \n",
    "  - Do we need both of them?\n",
    "  - If our model says `1stFlrSF` is very important and `TotalBsmtSF` is very unimportant, do we trust those values?\n",
    "  - Maybe `TotalBsmtSF` only \"becomes important\" if `1stFlrSF` is removed.\n",
    "  - Sometimes the opposite happens: a feature only becomes important if another feature is _added_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50638aea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature importances in linear models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d722c4fe",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- With linear regression we can look at the _coefficients_ for each feature.\n",
    "- Overall idea: predicted price = intercept + $\\sum_i$ coefficient i $\\times$ feature i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb940114",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor, Ridge())\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbe7687",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs = pd.DataFrame(\n",
    "    data=lr.named_steps[\"ridge\"].coef_, index=new_columns, columns=[\"Coefficient\"]\n",
    ")\n",
    "lr_coefs.sort_values(by=\"Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2a0070",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ordinal features\n",
    "\n",
    "- The ordinal features are easiest to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c71f6f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(ordinal_features_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbfb458",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"ExterQual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d99fa2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Increasing by one category of exterior quality (e.g. good -> excellent) increases the predicted price by $\\sim\\$4237$.\n",
    "  - Wow, that's a lot! \n",
    "  - Remember this is just what the model has learned. It doesn't tell us how the world works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e499f8e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "one_example = X_test[:1]\n",
    "one_example[[\"ExterQual\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d996a44",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's perturb the example and change `ExterQual` to `Ex`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a328f1",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "one_example_perturbed = one_example.copy()\n",
    "one_example_perturbed[\"ExterQual\"] = \"Ex\"\n",
    "one_example_perturbed[[\"ExterQual\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4222f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prediction on the original example: \", lr.predict(one_example))\n",
    "print(\"Prediction on the perturbed example: \", lr.predict(one_example_perturbed))\n",
    "print(\n",
    "    \"After changing ExterQual from Gd to Ex increased the prediction by: \",\n",
    "    lr.predict(one_example_perturbed) - lr.predict(one_example),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33e685f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "That's exactly the learned coefficient for `ExterQual`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196c44b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So our interpretation is correct! \n",
    "- Increasing by one category of exterior quality (e.g. good -> excellent) increases the predicted price by $\\sim\\$4237$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445171d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0ede4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What about the categorical features?\n",
    "- We have created a number of columns for each category with OHE and each category gets it's own coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27ed37c",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e8498e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs_landslope = lr_coefs[lr_coefs.index.str.startswith(\"LandSlope\")]\n",
    "lr_coefs_landslope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e007e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can talk about switching from one of these categories to another by picking a \"reference\" category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c13d57",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs_landslope - lr_coefs_landslope.loc[\"LandSlope_Gtl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd0b2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- If you change the category from `LandSlope_Gtl` to `LandSlope_Mod` the prediction price goes up by $\\sim\\$6950$\n",
    "- If you change the category from `LandSlope_Gtl` to `LandSlope_Sev` the prediction price goes down by $\\sim\\$8356$\n",
    "\n",
    "\n",
    "Note that this might not make sense in the real world but this is what our model decided to learn given this small amount of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be96e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "one_example = X_test[:1]\n",
    "one_example[['LandSlope']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779969e4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's perturb the example and change `LandSlope` to `Mod`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed92354",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "one_example_perturbed = one_example.copy()\n",
    "one_example_perturbed[\"LandSlope\"] = \"Mod\"  # Change Gtl to Mode\n",
    "one_example_perturbed[[\"LandSlope\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fa6c5a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How does the prediction change after changing `LandSlope` from `Gtl` to `Mod`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c17e48",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prediction on the original example: \", lr.predict(one_example))\n",
    "print(\"Prediction on the perturbed example: \", lr.predict(one_example_perturbed))\n",
    "print(\n",
    "    \"After changing ExterQual from Gd to Ex increased the prediction by: \",\n",
    "    lr.predict(one_example_perturbed) - lr.predict(one_example),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccdf79f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Our interpretation above is correct! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874cf4ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For example, the coefficient says that \"If the roof is made of clay or tile, the predicted price is \\\\$191K less\"?\n",
    "- Do we believe these interpretations??\n",
    "  - Do we believe this is how the predictions are being computed? Yes.\n",
    "  - Do we believe that this is how the world works? No. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3fb388",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs.sort_values(by=\"Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa72150",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{note}\n",
    "If you did `drop='first'` (we didn't) then you already have a reference class, and all the values are with respect to that one. The interpretation depends on whether we did `drop='first'`, hence the hassle.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c736fc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Interpreting coefficients of numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5aa82",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's look at coefficients of `PoolArea`, `LotFrontage`, `LotArea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211b6ac",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[[\"PoolArea\", \"LotFrontage\", \"LotArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e832686",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Intuition: \n",
    "\n",
    "- Tricky because numeric features are **scaled**! \n",
    "- **Increasing** `PoolArea` by 1 scaled unit **increases** the predicted price by $\\sim\\$2817$.\n",
    "- **Increasing** `LotArea` by 1 scaled unit **increases** the predicted price by $\\sim\\$5118$.\n",
    "- **Increasing** `LotFrontage` by 1 scaled unit **decreases** the predicted price by $\\sim\\$1582$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f66353",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Does that sound reasonable?\n",
    "\n",
    "- For `PoolArea` and `LotArea`, yes. \n",
    "- For `LotFrontage`, that's surprising. Something positive would have made more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194ba1c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It might be the case that `LotFrontage` is correlated with some other variable, which might have a larger positive coefficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab53491",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example showing how can we interpret coefficients of scaled features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c3136",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What's one scaled unit for `LotArea`? \n",
    "- The scaler subtracted the mean and divided by the standard deviation.\n",
    "- The division actually changed the scale! \n",
    "- For the unit conversion, we don't care about the subtraction, but only the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a81ca7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = preprocessor.named_transformers_[\"pipeline-1\"][\"standardscaler\"]\n",
    "lr_scales = pd.DataFrame(\n",
    "    data=np.sqrt(scaler.var_), index=numeric_features, columns=[\"Scale\"]\n",
    ")\n",
    "lr_scales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d22a36",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- It seems like `LotArea` was divided by 8994.471032 sqft. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38899cac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The coefficient tells us that if we increase the **scaled** `LotArea` by one scaled unit the price would go up by $\\approx\\$5118$. \n",
    "- One scaled unit represents $\\sim 8994$ sqft in the original scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f151115",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "X_test_enc = pd.DataFrame(\n",
    "    preprocessor.transform(X_test), index=X_test.index, columns=new_columns\n",
    ")\n",
    "one_ex_preprocessed = X_test_enc[:1]\n",
    "one_ex_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3435526",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "orig_pred = lr.named_steps[\"ridge\"].predict(one_ex_preprocessed.values)\n",
    "orig_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a335dfa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "one_ex_preprocessed_perturbed = one_ex_preprocessed.copy()\n",
    "one_ex_preprocessed_perturbed[\"LotArea\"] += 1  # we are adding one to the scaled LotArea\n",
    "one_ex_preprocessed_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb239ff",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "perturbed_pred = lr.named_steps[\"ridge\"].predict(one_ex_preprocessed_perturbed.values)\n",
    "perturbed_pred - orig_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10777d0c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Humans find it easier to think about features in their original scale.  \n",
    "- How can we interpret this coefficient in the original scale? \n",
    "- If I increase original `LotArea` by one square foot then the predicted price would go up by this amount: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3341f68b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "5118.0351607 / 8994.471032 # Coefficient learned on the scaled features / the scaling factor for this feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce33663",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary\n",
    "\n",
    "- Correlation among features might make coefficients completely uninterpretable. \n",
    "- Fairly straightforward to interpret coefficients of ordinal features. \n",
    "- In categorical features, it's often helpful to consider one category as a reference point and think about relative importance. \n",
    "- For numeric features, relative importance is meaningful after scaling.\n",
    "- You have to be careful about the scale of the feature when interpreting the coefficients. \n",
    "- Remember that explaining the model $\\neq$ explaining the data or explaining how the world works.  \n",
    "- The coefficients tell us only about the model and they might not accurately reflect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8920a3a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transparency and explainability of ML models: Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b55e5c2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Activity (~5 mins)\n",
    "\n",
    "Suppose you have a machine learning model which gives you a 98% cross-validation score (with the metric of your interest) and 97% test score on a reasonably sized train and test sets. Since you have impressive cross-validation and test scores, you decide to just trust the model and use it as a black box, ignoring why it's making certain predictions. \n",
    "\n",
    "Give some scenarios when this might or might not be problematic.\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4349fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why model transparency/interpretability? \n",
    "\n",
    "- Ability to interpret ML models is crucial in many applications such as banking, healthcare, and criminal justice.\n",
    "- It can be leveraged by domain experts to diagnose systematic errors and underlying biases of complex ML systems. \n",
    "\n",
    "![](img/shap_example.png)\n",
    "\n",
    "<!-- <img src=\"img/shap_example.png\" width=\"600\" height=\"600\"> -->\n",
    "    \n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e14241",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is model interpretability? \n",
    "\n",
    "- In this lecture, our definition of model interpretability will be looking at **feature importances**, i.e., exploring features which are important to the model. \n",
    "- There is more to interpretability than feature importances, but it's a good start!\n",
    "- Resources: \n",
    "    - [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/interpretability-importance.html)\n",
    "    - [Yann LeCun, Kilian Weinberger, Patrice Simard, and Rich Caruana: Panel debate on interpretability](https://vimeo.com/252187813)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a85a08",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "- Let's work with [the adult census data set](https://www.kaggle.com/uciml/adult-census-income) from the last lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_df_large = pd.read_csv(\"../data/adult.csv\")\n",
    "train_df, test_df = train_test_split(adult_df_large, test_size=0.2, random_state=42)\n",
    "train_df_nan = train_df.replace(\"?\", np.NaN)\n",
    "test_df_nan = test_df.replace(\"?\", np.NaN)\n",
    "train_df_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f148924",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"marital.status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"native.country\",\n",
    "]\n",
    "ordinal_features = [\"education\"]\n",
    "binary_features = [\"sex\"]\n",
    "drop_features = [\"race\", \"education.num\", \"fnlwgt\"]\n",
    "target_column = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0386d6cf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "education_levels = [\n",
    "    \"Preschool\",\n",
    "    \"1st-4th\",\n",
    "    \"5th-6th\",\n",
    "    \"7th-8th\",\n",
    "    \"9th\",\n",
    "    \"10th\",\n",
    "    \"11th\",\n",
    "    \"12th\",\n",
    "    \"HS-grad\",\n",
    "    \"Prof-school\",\n",
    "    \"Assoc-voc\",\n",
    "    \"Assoc-acdm\",\n",
    "    \"Some-college\",\n",
    "    \"Bachelors\",\n",
    "    \"Masters\",\n",
    "    \"Doctorate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc651d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "assert set(education_levels) == set(train_df[\"education\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb11d9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "tree_numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"))\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "ordinal_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OrdinalEncoder(categories=[education_levels], dtype=int),\n",
    ")\n",
    "\n",
    "binary_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(drop=\"if_binary\", dtype=int),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer, ordinal_features),\n",
    "    (binary_transformer, binary_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21415bae",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df_nan.drop(columns=[target_column])\n",
    "y_train = train_df_nan[target_column]\n",
    "\n",
    "X_test = test_df_nan.drop(columns=[target_column])\n",
    "y_test = test_df_nan[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954a634c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Do we have class imbalance? \n",
    "\n",
    "- There is class imbalance. But without any context, both classes seem equally important. \n",
    "- Let's use accuracy as our metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2cbed4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df_nan[\"income\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a230cd4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scoring_metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b8f05",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67acd008",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are going to use models outside sklearn. Some of them cannot handle categorical target values. So we'll convert them to integers using `LabelEncoder`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce34e5a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# encode categorical class values as integers for XGBoost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_num = label_encoder.fit_transform(y_train)\n",
    "y_test_num = label_encoder.transform(y_test)\n",
    "y_train_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e5f40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9a2a4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier()\n",
    "results[\"Dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train_num, return_train_score=True, scoring=scoring_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13dc24",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fd93f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=2000, random_state=123)\n",
    ")\n",
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=123))\n",
    "pipe_xgb = make_pipeline(\n",
    "    preprocessor, XGBClassifier(random_state=123, eval_metric=\"logloss\", verbosity=0)\n",
    ")\n",
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123, verbose=-1))\n",
    "classifiers = {\n",
    "    \"logistic regression\": pipe_lr,\n",
    "    \"random forest\": pipe_rf,\n",
    "    \"XGBoost\": pipe_xgb,\n",
    "    \"LightGBM\": pipe_lgbm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8320f3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for (name, model) in classifiers.items():\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "        model, X_train, y_train_num, return_train_score=True, scoring=scoring_metric\n",
    "    )\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78d43d9",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Logistic regression is giving reasonable scores but not the best ones. \n",
    "- XGBoost and LightGBM are giving us the best CV scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c556d462",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Often simple models (e.g., linear models) are interpretable but not very accurate.\n",
    "- Complex models (e.g., LightGBM) are more accurate but less interpretable. \n",
    "\n",
    "![](../img/shap_motivation.png)\n",
    "\n",
    "<!-- <img src=\"img/shap_motivation.png\" width=\"500\" height=\"500\"> -->\n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54565f8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Feature importances in linear models\n",
    "\n",
    "Let's create and fit a pipeline with preprocessor and logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656d2ec",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression(max_iter=2000, random_state=2))\n",
    "pipe_lr.fit(X_train, y_train_num);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfdca10",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "ohe_feature_names = (\n",
    "    pipe_rf.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    "    .tolist()\n",
    ")\n",
    "feature_names = (\n",
    "    numeric_features + ordinal_features + binary_features + ohe_feature_names\n",
    ")\n",
    "feature_names[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c0187",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"coefficient\": pipe_lr.named_steps[\"logisticregression\"].coef_.flatten().tolist(),\n",
    "    \"magnitude\": np.absolute(\n",
    "        pipe_lr.named_steps[\"logisticregression\"].coef_.flatten().tolist()\n",
    "    ),\n",
    "}\n",
    "coef_df = pd.DataFrame(data, index=feature_names).sort_values(\n",
    "    \"magnitude\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a17fba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "coef_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf343a2e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Increasing `capital.gain` is likely to push the prediction towards \">50k\" income class \n",
    "- Whereas occupation of private house service is likely to push the prediction towards \"<=50K\" income. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec0b9c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we get feature importances for non-linear models? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da5867",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model interpretability beyond linear models\n",
    "\n",
    "- We will be looking at interpretability in terms of feature importances. \n",
    "- Note that there is no absolute or perfect way to get feature importances. But it's useful to get some idea on feature importances. So we just try our best. \n",
    "\n",
    "We will be looking at three ways to get feature importances. \n",
    "\n",
    "- `sklearn`'s `feature_importances_` and `permutation_importance`\n",
    "- [eli5](https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html#lime-tutorial) (stands for \"explain like I'm 5\") \n",
    "- [SHAP](https://github.com/slundberg/shap)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80609420",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Activity (~5 mins)\n",
    "\n",
    "Linear models learn a coefficient associated with each feature which tells us the importance of the feature to the model. \n",
    "- What might be some reasonable ways to calculate feature importances of the following models? \n",
    "    - Decision trees    \n",
    "    - Linear SVMs\n",
    "    - KNNs, RBF SVMs\n",
    "- Suppose you have correlated features in your dataset. Do you need to be careful about this when you examine feature importances? \n",
    "\n",
    "Discuss with your neighbour."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b653da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do we have correlated features? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173dc130",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc = preprocessor.fit_transform(X_train).todense()\n",
    "corr_df = pd.DataFrame(X_train_enc, columns=feature_names).corr().abs()\n",
    "corr_df[corr_df == 1] = 0 # Set the diagonal to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5576f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Let's look at columns where any correlation number is > 0.80. \n",
    "- 0.80 is an arbitrary choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50529f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "high_corr = [column for column in corr_df.columns if any(corr_df[column] > 0.80)]\n",
    "print(high_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2980037",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Seems like there are some columns which are highly correlated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ae4b6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "corr_df['occupation_missing']['workclass_missing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f8784",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df['marital.status_Married-civ-spouse']['relationship_Husband']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9081cbe5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- When we look at the feature importances, we should be mindful of these correlated features. \n",
    "- Remember the limitations of looking at simple linear correlations. \n",
    "- You should probably investigate multi-colinearity with more sophisticated approaches (e.g., variance inflation factors (VIF)).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152e7848",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn`'s `feature_importances_` attribute vs `permutation_importance`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63760ae",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Feature importances can be \n",
    "    - algorithm dependent, i.e., calculated based on the information given by the model algorithm (e.g., gini importance) \n",
    "    - model agnostic (e.g., by measuring increase in prediction error after permuting feature values). \n",
    "    \n",
    "- Different measures give insight into different aspects of your data and model. \n",
    "\n",
    "> [Here](https://scikit-learn.org/stable/modules/permutation_importance.html#relation-to-impurity-based-importance-in-trees) you will find some drawbacks of using `feature_importances_` attribute  in the context of tree-based models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7c6d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision tree feature importances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261c79e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_dt = make_pipeline(preprocessor, DecisionTreeClassifier(max_depth=3))\n",
    "pipe_dt.fit(X_train, y_train_num);\n",
    "data = {\n",
    "    \"Importance\": pipe_dt.named_steps[\"decisiontreeclassifier\"].feature_importances_,\n",
    "}\n",
    "pd.DataFrame(data=data, index=feature_names,).sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67f3dd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "custom_plot_tree(pipe_dt.named_steps[\"decisiontreeclassifier\"], feature_names = feature_names, fontsize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b644d",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "def get_permutation_importance(model):\n",
    "    X_train_perm = X_train.drop(columns=[\"race\", \"education.num\", \"fnlwgt\"])\n",
    "    result = permutation_importance(model, X_train_perm, y_train_num, n_repeats=10, random_state=123)\n",
    "    perm_sorted_idx = result.importances_mean.argsort()\n",
    "    plt.boxplot(\n",
    "        result.importances[perm_sorted_idx].T,\n",
    "        vert=False,\n",
    "        labels=X_train_perm.columns[perm_sorted_idx],\n",
    "    )\n",
    "    plt.xlabel('Permutation feature importance')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e854dc28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's explore permutation importance. \n",
    "- For each feature this method evaluates the impact of permuting feature values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1075d00",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "get_permutation_importance(pipe_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c548b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Decision tree is primarily making all decisions based on three features: marital.status, education, and capital.gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec0460",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Random forest feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9f894b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=2))\n",
    "pipe_rf.fit(X_train, y_train_num);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83be4725",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Which features are driving the predictions the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe02669",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Importance\": pipe_rf.named_steps[\"randomforestclassifier\"].feature_importances_,\n",
    "}\n",
    "pd.DataFrame(\n",
    "    data=data,\n",
    "    index=feature_names,\n",
    ").sort_values(by=\"Importance\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f0d3d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.sum(pipe_rf.named_steps[\"randomforestclassifier\"].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8535094",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "get_permutation_importance(pipe_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28802139",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Random forest is using more features in the model compared to decision trees. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca393fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key point \n",
    "\n",
    "- Unlike the linear model coefficients, `feature_importances_` **do not have a sign**!\n",
    "  - They tell us about importance, but not an \"up or down\".\n",
    "  - Indeed, increasing a feature may cause the prediction to first go up, and then go down.\n",
    "  - This cannot happen in linear models, because they are linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab9a82",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we get feature importances for non `sklearn` models? \n",
    "\n",
    "- One way to do it is by using a tool called [`eli5`](https://eli5.readthedocs.io/en/latest/overview.html).\n",
    "\n",
    "Unfortunately, this is not compatible with the latest version of sklearn, which we are using. \n",
    "\n",
    "```\n",
    "conda install -c conda-forge eli5\n",
    "```\n",
    "\n",
    "- Another popular way is using `SHAP`. You can install it using the following in the course conda environment. \n",
    "\n",
    "```conda install -c conda-forge shap```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655c2e7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SHAP  (SHapley Additive exPlanations) introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13967b2",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Explaining a prediction \n",
    "\n",
    "![](../img/shap_example.png)\n",
    "<!-- <img src=\"img/shap_example.png\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a493c9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SHAP  (SHapley Additive exPlanations)\n",
    "- Based on the idea of shapely values. A shapely value is created for each example and each feature. \n",
    "- Can explain the prediction of an example by computing the contribution of each feature to the prediction. \n",
    "- Great visualizations \n",
    "- Support for different kinds of models; fast variants for tree-based models\n",
    "- Original paper: [Lundberg and Lee, 2017](https://arxiv.org/pdf/1705.07874.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54801b0e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Our focus\n",
    "- How to use it on our dataset?\n",
    "- How to generate and interpret plots created by SHAP? \n",
    "- We are not going to discuss how SHAP works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e6977",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SHAP on LGBM model \n",
    "\n",
    "- Let's try it out on our best performing LightGBM model. \n",
    "- You should have `shap` in the course conda environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7280fc",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_train).toarray(),\n",
    "    columns=feature_names,\n",
    "    index=X_train.index,\n",
    ")\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ea795",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "X_test_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_test).toarray(),\n",
    "    columns=feature_names,\n",
    "    index=X_test.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f789c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Create a shap explainer object \n",
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].fit(X_train_enc, y_train)\n",
    "lgbm_explainer = shap.TreeExplainer(pipe_lgbm.named_steps[\"lgbmclassifier\"])\n",
    "train_lgbm_shap_values = lgbm_explainer.shap_values(X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acac1e5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_lgbm_shap_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f379a28",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- For each example, each feature, and each class we have a SHAP value.\n",
    "- SHAP values tell us how to fairly distribute the prediction among features. \n",
    "- For classification it's a bit confusing. It gives SHAP matrix for all classes.\n",
    "- Let's stick to shap values for class 1, i.e., income > 50K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767ca70a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(train_lgbm_shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e5eee3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_lgbm_shap_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b3e238",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_lgbm_shap_values = lgbm_explainer.shap_values(X_test_enc)\n",
    "test_lgbm_shap_values[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988082ad",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SHAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576f2a6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21865b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Force plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2476c7f",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Most useful! \n",
    "- Let's try to explain predictions on a couple of examples from the test data. \n",
    "- I'm sampling some examples where target is <=50K and some examples where target is >50K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5b77b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "y_test_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56aca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "l50k_ind = y_test_reset[y_test_reset == \"<=50K\"].index.tolist()\n",
    "g50k_ind = y_test_reset[y_test_reset == \">50K\"].index.tolist()\n",
    "\n",
    "ex_l50k_index = l50k_ind[10]\n",
    "ex_g50k_index = g50k_ind[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5892b4f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explaining a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27f3fc6",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Imagine that you are given the following test example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a2b9fb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_test_enc.iloc[ex_l50k_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f816205b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You get the following hard prediction, which you are interested in explaining.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3beaf0ac",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc)[ex_l50k_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596e90c3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You can first look at `predict_proba` output to get a better understanding of model confidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6e415a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict_proba(X_test_enc)[ex_l50k_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232ac652",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The model seems quite confident. But if we want to know more, for example, which feature values are playing a role in this specific prediction, we can use SHAP force plots. \n",
    "- Remember that we have SHAP values per feature per example. We'll use these values to create SHAP force plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d91878e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    test_lgbm_shap_values[1][ex_l50k_index, :],\n",
    "    index=feature_names,\n",
    "    columns=[\"SHAP values\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c6873a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "SHAP will produce the following type of plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d22814",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1], # expected value for class 1. \n",
    "    test_lgbm_shap_values[1][ex_l50k_index, :], # SHAP values associated with the example we want to explain\n",
    "    X_test_enc.iloc[ex_l50k_index, :], # Feature vector of the example \n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc20eb76",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The raw model score is much smaller than the base value, which is reflected in the prediction of <= 50k class. \n",
    "- sex = 1.0, scaled age = 0.48 are pushing the prediction towards higher score. \n",
    "- education = 8.0, occupation_Other-service = 1.0 and marital.status_Married-civ-spouse = 0.0 are pushing\n",
    "the prediction towards lower score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2ec7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can get the raw model output by passing `raw_score=True` in `predict`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e69dd3",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc, raw_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31048745",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's the raw score of the example we are trying to explain?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f355ec",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc, raw_score=True)[ex_l50k_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76a135",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The score matches with what we see in the force plot. \n",
    "- The base score above is the mean raw score. Our example has a lower raw score compared to the average raw score and the force plot tries to explain which feature values are bringing this score to a lower value. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c330564",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Note: a nice thing about SHAP values is that the feature importances sum to the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e6819",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "test_lgbm_shap_values[1][ex_l50k_index, :].sum() + lgbm_explainer.expected_value[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29818fb7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now let's try to explain another prediction. \n",
    "- The hard prediction here is 1. \n",
    "- From the `predict_proba` output it seems like the model is not too confident about the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb0160",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc)[ex_g50k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a243f3b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict_proba(X_test_enc)[ex_g50k_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044df4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What's the raw score for this example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58065f4",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc, raw_score=True)[\n",
    "    ex_g50k_index\n",
    "]  # raw model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b75502",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][ex_g50k_index, :],\n",
    "    X_test_enc.iloc[ex_g50k_index, :],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440ddff",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Observations: \n",
    "\n",
    "- Everything is with respect to class 1 here. \n",
    "- The base value, i.e., the average raw score for class 1 is -2.336. \n",
    "- We see the forces that drive the prediction. \n",
    "- That is, we can see the main factors pushing it from the base value (average over the dataset) to this particular prediction. \n",
    "- Features that push the prediction to a higher value are shown in red. \n",
    "- Features that push the prediction to a lower value are shown in blue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30110a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Global feature importance using SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef2bb9b",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's look at the average SHAP values associated with each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eaa7f3a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "values = np.abs(train_lgbm_shap_values[1]).mean(\n",
    "    0\n",
    ")  # mean of shapely values in each column\n",
    "pd.DataFrame(data=values, index=feature_names, columns=[\"SHAP\"]).sort_values(\n",
    "    by=\"SHAP\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fb19ac",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2035a478",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"age\", train_lgbm_shap_values[1], X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e25249",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The plot above shows effect of `age` feature on the prediction. \n",
    "\n",
    "- Each dot is a single prediction for examples above.\n",
    "- The x-axis represents values of the feature age (scaled).\n",
    "- The y-axis is the SHAP value for that feature, which represents how much knowing that feature's value changes the output of the model for that example's prediction. \n",
    "- Lower values of age have smaller SHAP values for class \">50K\".\n",
    "- Similarly, higher values of age also have a bit smaller SHAP values for class \">50K\", which makes sense.  \n",
    "- There is some optimal value of age between scaled age of 1 which gives highest SHAP values for for class \">50K\". \n",
    "- Ignore the colour for now. The color corresponds to a second feature (education feature in this case) that may have an interaction effect with the feature we are plotting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886dd3ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summary plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07130946",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(train_lgbm_shap_values[1], X_train_enc, plot_size=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d060333",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The plot shows the most important features for predicting the class. It also shows the direction of how it's going to drive the prediction.  \n",
    "\n",
    "- Presence of the marital status of Married-civ-spouse seems to have bigger SHAP values for class 1 and absence seems to have smaller SHAP values for class 1. \n",
    "- Higher levels of education seem to have bigger SHAP values for class 1 whereas smaller levels of education have smaller SHAP values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b099df8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(train_lgbm_shap_values[1], X_train_enc, plot_type=\"bar\", plot_size=(8, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0b2143",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can think of this as global feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ccb58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here, we explore SHAP's TreeExplainer. It also provides explainer for different kinds of models. \n",
    "\n",
    "- [TreeExplainer](https://shap.readthedocs.io/en/latest/) (supports XGBoost, CatBoost, LightGBM) \n",
    "- [DeepExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.DeepExplainer) (supports deep-learning models)\n",
    "- [KernelExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.KernelExplainer) (supports kernel-based models)\n",
    "- [GradientExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.GradientExplainer) (supports Keras and Tensorflow models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd6997",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Can also be used to explain text classification and image classification \n",
    "- Example: In the picture below, red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. \n",
    "\n",
    "![](../img/shap_image_explainer.png)\n",
    "<!-- <img src=\"img/shap_image_explainer.png\" width=\"600\" height=\"600\"> -->\n",
    "    \n",
    "[Source](https://github.com/slundberg/shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5206904c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you're not already impressed, keep in mind:\n",
    "\n",
    "- So far we've only used sklearn models.\n",
    "- Most sklearn models have some built-in measure of feature importances.\n",
    "- On many tasks we need to move beyond sklearn, e.g. LightGBM, deep learning.\n",
    "- These tools work on other models as well, which makes them extremely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c8dd7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why do we want this information?\n",
    "\n",
    "Possible reasons:\n",
    "\n",
    "- Identify features that are not useful and maybe remove them.\n",
    "- Get guidance on what new data to collect.\n",
    "  - New features related to useful features -> better results.\n",
    "  - Don't bother collecting useless features -> save resources.\n",
    "- Help explain why the model is making certain predictions. \n",
    "  - Debugging, if the model is behaving strangely.\n",
    "  - Regulatory requirements.\n",
    "  - Fairness / bias. See [this](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Explaining%20quantitative%20measures%20of%20fairness.html). \n",
    "  - Keep in mind this can be used on **deployment** predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1438ee91",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here are some guidelines and important points to remember when you work on a prediction problem where you also want to understand which features are influencing the predictions.  \n",
    "\n",
    "- Examine multicoliniarity in your dataset using methods such as VIF. \n",
    "- If you observe high correlations in your dataset, either get rid of redundant features or be mindful of these correlations during interpretation. \n",
    "- Be mindful that feature relevance is not clearly defined. Adding/removing features can change feature importance/unimportance. Also, feature importances do not give us causal relationships. See [this optional section](https://pages.github.ubc.ca/mds-2022-23/DSCI_573_feat-model-select_students/lectures/04_feat-importances-selection.html#optional-problems-with-feature-selection) from Lecture 4. \n",
    "- Most of the models we use in ML are regularized models. With L2 regularization, the feature importances are distributed evenly among correlated features. With L1 regularization, one of the correlated features gets a high importance and the other gets a lower importance.\n",
    "- Don't be overconfident. Always take feature importance values with a grain of salt.  "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:cpsc330]",
   "language": "python",
   "name": "conda-env-cpsc330-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
